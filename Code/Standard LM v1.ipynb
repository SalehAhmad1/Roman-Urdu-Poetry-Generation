{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saleh Ahmad\n",
    "i200605@nu.edu.pk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Laplace_LM_Roman_Urdu():\n",
    "    def __init__(self,ngram_type=None):\n",
    "        if ngram_type == None:\n",
    "            self.ngram = 2\n",
    "        else:\n",
    "            self.ngram = ngram_type\n",
    "\n",
    "    def fit(self,TrainCorpusVocab): #TrainCorpus = all tokens in order of occurence\n",
    "        Vocabs = defaultdict(int)\n",
    "        UniqueVocabSize = defaultdict(int)\n",
    "        VocabSize = defaultdict(int)\n",
    "        for ngram in range(self.ngram,0,-1):\n",
    "            IdxToken = 0\n",
    "            ngrams = defaultdict(int)\n",
    "            Len_Vocab = len(TrainCorpusVocab)\n",
    "            while IdxToken < Len_Vocab-ngram+1:\n",
    "                current_n_gram = TrainCorpusVocab[IdxToken]\n",
    "                for idx in range(1,ngram):\n",
    "                    current_n_gram += ' ' + TrainCorpusVocab[IdxToken+idx]\n",
    "                \n",
    "                ngrams[current_n_gram] += 1\n",
    "                IdxToken += 1\n",
    "\n",
    "            ngrams = {k:v for k,v in ngrams.items() if v > 0}\n",
    "            Vocabs[ngram] = ngrams\n",
    "            UniqueVocabSize[ngram] = len(ngrams)\n",
    "            VocabSize[ngram] = sum(ngrams.values())\n",
    "        \n",
    "        self.Vocabs = Vocabs\n",
    "        self.UniqueVocabSizes = UniqueVocabSize\n",
    "        self.VocabSizes = VocabSize\n",
    "\n",
    "    def __Get_Random_Length_Of_Verse(self):\n",
    "        #generate random length b/w 5 and 9\n",
    "        return random.randint(6,8)\n",
    "\n",
    "    def __Get_Random_Starting_Token(self,s_tokens):\n",
    "        #generate random starting token from s_tokens which is a list of starting tokens\n",
    "        return random.choice(s_tokens)\n",
    "    \n",
    "    def __Print_Vocab(self):\n",
    "        print(self.Vocabs)\n",
    "\n",
    "    def __Get_Next_Word_From_Candidate(self,Sequence):\n",
    "        ngram_ToUse = len(Sequence.split()) + 1\n",
    "        Candidates = [ngram for ngram in self.Vocabs[ngram_ToUse].keys() if ' '.join(ngram.split()[0:-1]) == Sequence]\n",
    "        if len(Candidates) == 0:\n",
    "            return self.__Get_Next_Word_From_Candidate(' '.join(Sequence.split()[1:])) #BackOff\n",
    "        else:\n",
    "            Probs_Of_Candidates = np.zeros_like(Candidates)\n",
    "            for idxCandidate,candidate in enumerate(Candidates):\n",
    "                Probs_Of_Candidates[idxCandidate] = (self.Vocabs[ngram_ToUse][candidate] + 1) / ((self.VocabSizes[self.ngram] + self.UniqueVocabSizes[self.ngram]))\n",
    "            return Candidates[np.argmax(Probs_Of_Candidates)].split()[-1] #return max prob and the next word\n",
    "\n",
    "    def __ReverseBackOff(self,sequence): #Doesn't take into account the importance of any wordother than the first (Reverse Back Off Kinda?) (1....n-1 grams)\n",
    "        '''Initially the length of the sequence will be one. It will try to complete the sequence to the self.ngrams-1 length mark.\n",
    "        However all words added after the first one will be of equal importance. What it should do is that it should take into account\n",
    "          the importance of the first word and then add the next word based on the importance of the first word and so on.\n",
    "        '''\n",
    "        ngram_To_Use = self.ngram - 1\n",
    "        Candidates = [ngram for ngram in self.Vocabs[ngram_To_Use].keys() if ngram.split()[0] == sequence]\n",
    "        Probs_Of_Candidates = np.zeros_like(Candidates)\n",
    "        for idxCandidate,candidate in enumerate(Candidates):\n",
    "            Probs_Of_Candidates[idxCandidate] = (self.Vocabs[ngram_To_Use][candidate] + 1) / ((self.VocabSizes[self.ngram] + self.UniqueVocabSizes[self.ngram]))\n",
    "        return ' '.join(Candidates[np.argmax(Probs_Of_Candidates)].split()[1:]) #return max prob and the next word\n",
    "\n",
    "\n",
    "    def predict(self,StartingTokens): #StartingTokens = list of repetitive tokens, can extract unique and count\n",
    "        '''\n",
    "        Prediction format\n",
    "        1 stanza = 14 verses (1 * 14)\n",
    "        1 verse = 6 - 8 words\n",
    "        '''        \n",
    "\n",
    "        Verse = ''\n",
    "        for idxVerse in range(0,14):\n",
    "            CurrentVerse = ''\n",
    "            StartingTokenOfThisVerse = self.__Get_Random_Starting_Token(StartingTokens) #Verse starts with a random starting token, length is 1 right now\n",
    "            VerseLength = self.__Get_Random_Length_Of_Verse()\n",
    "            Sequence = StartingTokenOfThisVerse\n",
    "            CurrentVerse += Sequence\n",
    "\n",
    "            #Length is less than what is required by self.ngras\n",
    "            if self.ngram > 2 and len(CurrentVerse.split()) == 1:\n",
    "                NextWords = self.__ReverseBackOff(Sequence)\n",
    "                Sequence += ' ' + NextWords\n",
    "                CurrentVerse = Sequence\n",
    "\n",
    "            #Now the sequence is ready for self.ngram probs\n",
    "            for idxWord in range(len(CurrentVerse.split()),VerseLength):\n",
    "                NextWords = self.__Get_Next_Word_From_Candidate(Sequence)\n",
    "                Sequence = ' '.join(Sequence.split()[1:] + ([NextWords]))\n",
    "                CurrentVerse += ' ' + NextWords\n",
    "                \n",
    "            CurrentVerse += '\\n'\n",
    "            if idxVerse != 0 and idxVerse % 2 == 1:\n",
    "                CurrentVerse += '\\n'\n",
    "            Verse += CurrentVerse\n",
    "        return Verse \n",
    "\n",
    "    def Print_Details(self):\n",
    "        print('All ngrams',self.Vocabs.keys())\n",
    "        for ngram in self.Vocabs.keys():\n",
    "            print('ngram:',ngram)\n",
    "            print('ngrams:',list(self.Vocabs[ngram].keys())[0:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All ngrams dict_keys([4, 3, 2, 1])\n",
      "ngram: 4\n",
      "ngrams: ['gulon mein rang bharay', 'mein rang bharay bad', 'rang bharay bad nobahar', 'bharay bad nobahar chalay', 'bad nobahar chalay chalay', 'nobahar chalay chalay bhi', 'chalay chalay bhi aao', 'chalay bhi aao ke', 'bhi aao ke gulshan', 'aao ke gulshan ka']\n",
      "ngram: 3\n",
      "ngrams: ['gulon mein rang', 'mein rang bharay', 'rang bharay bad', 'bharay bad nobahar', 'bad nobahar chalay', 'nobahar chalay chalay', 'chalay chalay bhi', 'chalay bhi aao', 'bhi aao ke', 'aao ke gulshan']\n",
      "ngram: 2\n",
      "ngrams: ['gulon mein', 'mein rang', 'rang bharay', 'bharay bad', 'bad nobahar', 'nobahar chalay', 'chalay chalay', 'chalay bhi', 'bhi aao', 'aao ke']\n",
      "ngram: 1\n",
      "ngrams: ['gulon', 'mein', 'rang', 'bharay', 'bad', 'nobahar', 'chalay', 'bhi', 'aao', 'ke']\n"
     ]
    }
   ],
   "source": [
    "ObjLM = Laplace_LM_Roman_Urdu(4)\n",
    "\n",
    "AllTokens = np.load('../Data/All_Tokens_Roman_Urdu.npy')\n",
    "AllTokens = [token.lower() for token in AllTokens]\n",
    "StartingTokens = np.load('../Data/Starting_Tokens_Roman_Urdu.npy')\n",
    "StartingTokens = [token.lower() for token in StartingTokens]\n",
    "ObjLM.fit(AllTokens)\n",
    "\n",
    "ObjLM.Print_Details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pred = ObjLM.predict(StartingTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mein le ke niklon ga –apne darmandah karwan\n",
      "hum bhi gham e ishhq ko par ab\n",
      "\n",
      "garmi kahan kuch khayaal aaya tha wehshat ka\n",
      "go sab ko baham saghar o\n",
      "\n",
      "meri na-ash ko kheinchain phiro ke mein\n",
      "chaand se maand sitaron ne kaha aakhir shab\n",
      "\n",
      "ik shore uthaya ghalib aah jo\n",
      "daam شنیدن jis qader chahay bichaya mudda anqa\n",
      "\n",
      "apni khamshi gungi goya har simt se\n",
      "tamasha marey agay ik khail hai\n",
      "\n",
      "sab ki nazrain bacha ke dekh\n",
      "lartay hain aur haath mein talwar bhi\n",
      "\n",
      "iqbal ka thikana abhi wohi kefiyat hai is\n",
      "burhenapa payi wohi rahay gi magar naya khaar\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(Pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "90857eea415e892742aa58aac1803481ac02e4a0a6170a8a09c25267b7e414aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
