{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saleh Ahmad\n",
    "i200605@nu.edu.pk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bidirectional_Bigram_LM_Roman_Urdu():\n",
    "    '''\n",
    "    I have a question, does a bidirectional look for left word when predicting from left and right word when predicting from right separately?\n",
    "    or does it also look at left of \"to be predicted word\" and also to the end/right when predicting from left, same for when predicting from right.\n",
    "\n",
    "    Sipposing this one looks at left only for predicting from left to right and right only when predicting from right to left.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.ngram = 2\n",
    "\n",
    "    def fit(self,TrainCorpusVocab): #TrainCorpus = all tokens in order of occurence\n",
    "        Vocabs = defaultdict(int)\n",
    "        UniqueVocabSize = defaultdict(int)\n",
    "        VocabSize = defaultdict(int)\n",
    "        for ngram in range(self.ngram,0,-1):\n",
    "            IdxToken = 0\n",
    "            ngrams = defaultdict(int)\n",
    "            Len_Vocab = len(TrainCorpusVocab)\n",
    "            while IdxToken < Len_Vocab-ngram+1:\n",
    "                current_n_gram = TrainCorpusVocab[IdxToken]\n",
    "                for idx in range(1,ngram):\n",
    "                    current_n_gram += ' ' + TrainCorpusVocab[IdxToken+idx]\n",
    "                \n",
    "                ngrams[current_n_gram] += 1\n",
    "                IdxToken += 1\n",
    "\n",
    "            ngrams = {k:v for k,v in ngrams.items() if v > 0}\n",
    "            Vocabs[ngram] = ngrams\n",
    "            UniqueVocabSize[ngram] = len(ngrams)\n",
    "            VocabSize[ngram] = sum(ngrams.values())\n",
    "        \n",
    "        self.Vocabs = Vocabs\n",
    "        self.UniqueVocabSizes = UniqueVocabSize\n",
    "        self.VocabSizes = VocabSize\n",
    "\n",
    "    def __Get_Random_Length_Of_Verse(self):\n",
    "        return 8\n",
    "\n",
    "    def __Get_Random_Ending_Token(self,e_tokens):\n",
    "        e_tokens = np.unique(e_tokens)\n",
    "        return random.choice(e_tokens)\n",
    "    \n",
    "    def __Get_Random_Starting_Token(self,s_tokens):\n",
    "        return random.choice(s_tokens)\n",
    "\n",
    "    def __Get_Probability_Based_Ending_Tokens(self,s_tokens,Chosen_Starting_Token,e_tokens):\n",
    "        '''\n",
    "        Given that a starting token has been chosen randomly, I think that the ending token should be based upon which has the most probability.\n",
    "        Do I also need a vocab for this? Let's Try.\n",
    "        '''\n",
    "        indices = np.array([idx for idx,token in enumerate(s_tokens) if token == Chosen_Starting_Token])\n",
    "        Ending_Token_Candidates = [e_tokens[idx] for idx in indices]\n",
    "        unique,count = np.unique(Ending_Token_Candidates,return_counts=True)\n",
    "        Chosen_Ending_Token = unique[np.argmax(count)]\n",
    "        return Chosen_Ending_Token\n",
    "    \n",
    "    def __Print_Vocab(self):\n",
    "        print(self.Vocabs)\n",
    "\n",
    "    def __Get_Next_Word_From_Candidate_Right_To_Left(self,Sequence):\n",
    "        NGramToUse = self.ngram\n",
    "        Candidates = [ngram for ngram in self.Vocabs[NGramToUse].keys() if ''.join(ngram.split()[-1]) == Sequence]\n",
    "                \n",
    "        if len(Candidates) == 0:\n",
    "            ToReturn = random.choice(list(self.Vocabs[NGramToUse-1].keys())[:10])\n",
    "            return ToReturn\n",
    "        else:\n",
    "            Probability_Of_Candidates = np.zeros_like(Candidates)\n",
    "            for idx,candidate in enumerate(Candidates):\n",
    "                Probability_Of_Candidates[idx] = (self.Vocabs[NGramToUse][candidate] + 1) / (self.VocabSizes[self.ngram] + self.UniqueVocabSizes[self.ngram])\n",
    "            return Candidates[np.argmax(Probability_Of_Candidates)].split()[-2]\n",
    "\n",
    "    def __Get_Next_Word_From_Candidate_Left_To_Right(self,Sequence):\n",
    "        ngram_ToUse = self.ngram\n",
    "        Candidates = [ngram for ngram in self.Vocabs[ngram_ToUse].keys() if ' '.join(ngram.split()[0:-1]) == Sequence]\n",
    "\n",
    "        if len(Candidates) == 0:\n",
    "            ToReturn = random.choice(list(self.Vocabs[ngram_ToUse-1].keys())[:10])\n",
    "            return ToReturn\n",
    "        else:\n",
    "            Probs_Of_Candidates = np.zeros_like(Candidates)\n",
    "            for idxCandidate,candidate in enumerate(Candidates):\n",
    "                Probs_Of_Candidates[idxCandidate] = (self.Vocabs[ngram_ToUse][candidate] + 1) / (self.VocabSizes[self.ngram] + self.UniqueVocabSizes[self.ngram])\n",
    "            return Candidates[np.argmax(Probs_Of_Candidates)].split()[-1] #return max prob and the next word\n",
    "\n",
    "    def predict(self,StartingTokens,EndingTokens): #StartingTokens = list of repetitive tokens, can extract unique and count\n",
    "        '''\n",
    "        Prediction format\n",
    "        1 stanza = 14 verses (1 * 14)\n",
    "        1 verse = 6 - 8 words\n",
    "        '''        \n",
    "\n",
    "        Verse = ''\n",
    "        for idxVerse in range(0,14):\n",
    "            StartingTokenOfThisVerse = self.__Get_Random_Starting_Token(StartingTokens)\n",
    "            EndingTokenOfThisVerse =  self.__Get_Probability_Based_Ending_Tokens(StartingTokens,StartingTokenOfThisVerse,EndingTokens)\n",
    "            LengthOfVerse = self.__Get_Random_Length_Of_Verse()\n",
    "            FirstHalf = int(LengthOfVerse/2)-1\n",
    "            SecondHalf = FirstHalf-1\n",
    "\n",
    "            VerseLeftToRight = StartingTokenOfThisVerse\n",
    "            Sequence = StartingTokenOfThisVerse\n",
    "            #From Left To Right\n",
    "            for idx in range(0,FirstHalf):\n",
    "                NextWord = self.__Get_Next_Word_From_Candidate_Left_To_Right(Sequence)\n",
    "                VerseLeftToRight += ' ' + NextWord\n",
    "                Sequence = NextWord\n",
    "\n",
    "            Sequence = EndingTokenOfThisVerse\n",
    "            VerseRightToLeft = EndingTokenOfThisVerse\n",
    "            #From Right To Left\n",
    "            for idx in range(0,SecondHalf):\n",
    "                PrevWord = self.__Get_Next_Word_From_Candidate_Right_To_Left(Sequence)\n",
    "                VerseRightToLeft = PrevWord + ' ' + VerseRightToLeft\n",
    "                Sequence = PrevWord\n",
    "\n",
    "            Verse += VerseLeftToRight + ' ' + VerseRightToLeft + '\\n'\n",
    "            if idxVerse != 0 and idxVerse % 2 == 1:\n",
    "                Verse += '\\n'\n",
    "        return Verse\n",
    "\n",
    "    def Print_Details(self):\n",
    "        print('All ngrams',self.Vocabs.keys())\n",
    "        for ngram in self.Vocabs.keys():\n",
    "            print('ngram:',ngram)\n",
    "            print('ngrams:',list(self.Vocabs[ngram].keys())[0:10])\n",
    "            print('priors:',list(self.Priors[ngram].keys())[0:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "AllTokens = np.load('../Data/All_Tokens_Roman_Urdu.npy')\n",
    "AllTokens = [token.lower() for token in AllTokens]\n",
    "EndingTokens = np.load('../Data/Ending_Tokens_Roman_Urdu.npy')\n",
    "EndingTokens = [token.lower() for token in EndingTokens]\n",
    "StartingTokens = np.load('../Data/Starting_Tokens_Roman_Urdu.npy')\n",
    "StartingTokens = [token.lower() for token in StartingTokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ObjLM = Bidirectional_Bigram_LM_Roman_Urdu()\n",
    "ObjLM.fit(AllTokens)\n",
    "# ObjLM.Print_Details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rehne walo kkhuda ne se aati agay\n",
      "samnay tairay koochy se aaye nah aaye\n",
      "\n",
      "deewangi se kuch aisi dil hi nahi\n",
      "suna hai shab mein kis ne aakhir\n",
      "\n",
      "taaraaj kawish gham rozgaar hai sun-hwa asad\n",
      "yeh kis ne ki apna bana hai\n",
      "\n",
      "ik shore uthaya ghalib se aati agay\n",
      "qanaat nah kyun kar bana hai par\n",
      "\n",
      "deewana par yeh kis dil hi nahi\n",
      "aatish khamosh ki hain dil guzar gaya\n",
      "\n",
      "yeh kis ne ki apna bana hai\n",
      "ghalib tujhe hum pay ne ki hain\n",
      "\n",
      "khumyaza hon is pay nah aaya tha\n",
      "aah mein le ga hi se nikla\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Pred = ObjLM.predict(StartingTokens,EndingTokens)\n",
    "print(Pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "90857eea415e892742aa58aac1803481ac02e4a0a6170a8a09c25267b7e414aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
